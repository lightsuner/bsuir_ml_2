{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 1.**\n",
    "\n",
    "Реализуйте полносвязную нейронную сеть с помощью библиотеки Tensor Flow. В качестве алгоритма оптимизации можно использовать, например, стохастический градиент (Stochastic Gradient Descent, SGD). Определите количество скрытых слоев от 1 до 5, количество нейронов в каждом из слоев до нескольких сотен, а также их функции активации (кусочно-линейная, сигмоидная, гиперболический тангенс и т.д.).\n",
    "\n",
    "**Задание 2.**\n",
    "\n",
    "Как улучшилась точность классификатора по сравнению с логистической регрессией?\n",
    "\n",
    "**Задание 3.**\n",
    "\n",
    "Используйте регуляризацию и метод сброса нейронов (dropout) для борьбы с переобучением. Как улучшилось качество классификации?\n",
    "\n",
    "**Задание 4.**\n",
    "\n",
    "Воспользуйтесь динамически изменяемой скоростью обучения (learning rate). Наилучшая точность, достигнутая с помощью данной модели составляет 97.1%. Какую точность демонстрирует Ваша реализованная модель?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T14:39:55.807473Z",
     "start_time": "2020-02-17T14:39:55.804316Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:36:52.701252Z",
     "start_time": "2020-02-17T17:36:50.354990Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy.io\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T14:40:06.542593Z",
     "start_time": "2020-02-17T14:40:06.529938Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T14:40:08.310694Z",
     "start_time": "2020-02-17T14:40:08.308397Z"
    }
   },
   "outputs": [],
   "source": [
    "large_dataset_path = '../../lab_1/src/notMNIST_large_clean.mat'\n",
    "small_dataset_path = '../../lab_1/src/notMNIST_small_uniq.mat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T14:40:08.727566Z",
     "start_time": "2020-02-17T14:40:08.724996Z"
    }
   },
   "outputs": [],
   "source": [
    "chars = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T14:40:09.444010Z",
     "start_time": "2020-02-17T14:40:09.436609Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(dataset, records=None):\n",
    "    data = list()\n",
    "    labels = list()\n",
    "\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(chars)\n",
    "    \n",
    "    if records:\n",
    "        one = int(records / len(chars))\n",
    "        #check\n",
    "        for ch in chars:\n",
    "            ch_len = len(dataset[ch])\n",
    "            assert ch_len >= one, f'\"{ch}\" has {ch_len} items but required {one}'\n",
    "        #print(one)\n",
    "        for ch in chars:\n",
    "            indexes = np.random.choice(len(dataset[ch]), one)\n",
    "            picked_elements = dataset[ch][indexes] / 255\n",
    "            data.extend(picked_elements)\n",
    "            labels.extend(np.tile(lb.transform([ch])[0], (one, 1)))\n",
    "    else:\n",
    "        for ch in chars:\n",
    "            data.extend(dataset[ch]/255)\n",
    "            labels.extend(np.tile(lb.transform([ch])[0], (len(dataset[ch]), 1)))\n",
    "    \n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return resample(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T14:40:11.239840Z",
     "start_time": "2020-02-17T14:40:10.909448Z"
    }
   },
   "outputs": [],
   "source": [
    "small_data = scipy.io.loadmat(small_dataset_path)\n",
    "test_X, test_y = prepare_dataset(small_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T14:40:26.043657Z",
     "start_time": "2020-02-17T14:40:12.134669Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 784)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_data = scipy.io.loadmat(large_dataset_path)\n",
    "train_X, train_y = prepare_dataset(large_data, 400000)\n",
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:34:10.495920Z",
     "start_time": "2020-02-17T17:34:10.490582Z"
    }
   },
   "outputs": [],
   "source": [
    "g_epochs = 150\n",
    "g_batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T14:40:26.082984Z",
     "start_time": "2020-02-17T14:40:26.046614Z"
    }
   },
   "outputs": [],
   "source": [
    "class Lab2Model():\n",
    "    def __init__(self, var_scope, layers_info, reg=0.0, dropout=0.0, lr=0.003, adaptive_lr=False):\n",
    "        self.var_scope = var_scope\n",
    "        self.layers_info = layers_info\n",
    "        self.layers_count = len(layers_info)\n",
    "        self.regularization = reg\n",
    "        self.dropout = dropout\n",
    "        self.learning_rate = lr\n",
    "        self.learning_rate_adaptive = adaptive_lr\n",
    "        self.session = None\n",
    "    \n",
    "    def __del__(self): \n",
    "        if self.session:\n",
    "            self.session.close()\n",
    "        \n",
    "    def compile(self):\n",
    "        self.input = tf.placeholder(tf.float32, shape=[None, self.layers_info[0]])\n",
    "        self.expected_output = tf.placeholder(tf.float32, shape=[None, self.layers_info[-1]])\n",
    "        \n",
    "        self.hidden_layers = {}\n",
    "        self.hidden_layers_W = {}\n",
    "        self.hidden_layers_b = {}\n",
    "        \n",
    "        self.dropout_rate = tf.placeholder(tf.float32)\n",
    "        \n",
    "        prev_layer = self.input\n",
    "        \n",
    "        for layer_idx in self.__hidden_layer_iter():            \n",
    "            units = self.layers_info[layer_idx]\n",
    "            self.hidden_layers[layer_idx] = self.__fc_layer(layer_idx, prev_layer, units)\n",
    "            prev_layer = self.hidden_layers[layer_idx]\n",
    "        \n",
    "        self.output_layer = self.__fc_layer(self.layers_count - 1, prev_layer, self.layers_info[-1], activ=None)\n",
    "        \n",
    "        self.loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.expected_output, logits=self.output_layer)\n",
    "        )\n",
    "        \n",
    "        if self.regularization:\n",
    "            reg_sum = 0\n",
    "            for layer_idx in self.__hidden_layer_iter():  \n",
    "                reg_sum += tf.nn.l2_loss(self.hidden_layers_W[layer_idx])\n",
    "            \n",
    "            self.loss = tf.reduce_mean(self.loss + reg_sum * self.regularization)\n",
    "        \n",
    "        self.lr = tf.placeholder(tf.float32, shape=[])\n",
    "        \n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(\n",
    "            learning_rate=self.learning_rate\n",
    "        ).minimize(self.loss)\n",
    "        \n",
    "        self.prediction = tf.nn.softmax(self.output_layer)\n",
    "        self.accuracy = tf.reduce_mean(\n",
    "            tf.cast(\n",
    "                tf.equal(tf.argmax(self.prediction, 1), tf.argmax(self.expected_output, 1)),\n",
    "                tf.float32,\n",
    "            )\n",
    "        )\n",
    "                \n",
    "        self.session = tf.Session()\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "        \n",
    "        self.history = {\n",
    "            'acc_train': [],\n",
    "            'acc_valid': [],\n",
    "            'loss_train': [],\n",
    "            'loss_valid': []\n",
    "        }\n",
    "    \n",
    "    def fit(self, x=None, y=None, batch_size=64, epochs=1):\n",
    "        \n",
    "        valid_size = 0.3\n",
    "        \n",
    "        if len(y) * valid_size > 20_000:\n",
    "            valid_size = 20_000\n",
    "        \n",
    "        x_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size=valid_size, random_state=50)\n",
    "        \n",
    "        print(f'Train size: {len(y_train)},\\t Valid size: {len(y_valid)}')\n",
    "        \n",
    "        iterations = int(len(y_train) / batch_size)\n",
    "        \n",
    "        display_info = int(iterations / 2)\n",
    "        \n",
    "        print(f'---------------------------------------------------------------')\n",
    "        print(f'Epochs: {epochs}\\t| Iterations: {iterations}\\t| Batch: {batch_size}')\n",
    "        print(f'---------------------------------------------------------------')\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print('Epoch: {}'.format(epoch + 1))\n",
    "            \n",
    "            x_train_epoch, y_train_epoch = resample(x_train, y_train)\n",
    "            \n",
    "            lr = self.__next_lr(epoch, epochs)\n",
    "            #print(lr)\n",
    "            \n",
    "            for iteration in range(iterations):\n",
    "                x_batch, y_batch = self.__next_batch(x_train_epoch, y_train_epoch, batch_size, iteration)\n",
    "\n",
    "                feed_data = { \n",
    "                    self.input: x_batch, \n",
    "                    self.expected_output: y_batch, \n",
    "                    self.lr: lr, \n",
    "                    self.dropout_rate: self.dropout\n",
    "                }\n",
    "                \n",
    "                self.session.run(self.optimizer, feed_dict=feed_data)\n",
    "                \n",
    "                #if iteration % display_info == 0:\n",
    "                #    info_feed_data = { self.input: x_batch, self.expected_output: y_batch, self.dropout_rate: 0}\n",
    "                #    loss_batch, acc_batch = self.session.run([self.loss, self.accuracy], feed_dict=info_feed_data)\n",
    "                #    print(f'Itr {iteration}:\\t loss={loss_batch:.2f},\\t acc={acc_batch:.01%}')\n",
    "        \n",
    "        \n",
    "            feed_data_train = { self.input: x_train, self.expected_output: y_train, self.dropout_rate: 0}\n",
    "            loss_train, acc_train = self.session.run([self.loss, self.accuracy], feed_dict=feed_data_train)\n",
    "\n",
    "            self.history['acc_train'].append(acc_train)\n",
    "            self.history['loss_train'].append(loss_train)\n",
    "\n",
    "            feed_data_valid = { self.input: x_valid, self.expected_output: y_valid, self.dropout_rate: 0}\n",
    "            loss_valid, acc_valid = self.session.run([self.loss, self.accuracy], feed_dict=feed_data_valid)\n",
    "\n",
    "            self.history['acc_valid'].append(acc_valid)\n",
    "            self.history['loss_valid'].append(loss_valid)\n",
    "\n",
    "            print('---------------------------------------------------------')\n",
    "            print('---------------------------------------------------------')\n",
    "            print(f'train loss: {loss_train:.2f}, train acc: {acc_train:.01%}')\n",
    "            print(f'valid loss: {loss_valid:.2f}, valid acc: {acc_valid:.01%}')\n",
    "            print('---------------------------------------------------------')\n",
    "            print('---------------------------------------------------------')\n",
    "    \n",
    "    def predict(self, x=None):\n",
    "        feed_data = { self.input: x, self.dropout_rate: 0}\n",
    "        return self.session.run(self.prediction, feed_dict=feed_data)\n",
    "    \n",
    "    def evaluate(self, x=None, y=None):\n",
    "        feed_data = { self.input: x, self.expected_output: y, self.dropout_rate: 0}\n",
    "        return self.session.run([self.loss, self.accuracy], feed_dict=feed_data)\n",
    "    \n",
    "    def __hidden_layer_iter(self):\n",
    "        return range(1, self.layers_count - 1)\n",
    "    \n",
    "    def __next_lr(self, current_iteration, iterations_count):\n",
    "        if not self.learning_rate_adaptive:\n",
    "            return self.learning_rate        \n",
    "        \n",
    "        adaptive_rate = 200\n",
    "\n",
    "        return self.learning_rate * adaptive_rate ** (1-(current_iteration / iterations_count))\n",
    "    \n",
    "    def __next_batch(self, x, y, batch_size, iteration):\n",
    "        start = iteration * batch_size\n",
    "        end = (iteration + 1) * batch_size\n",
    "        \n",
    "        return x[start:end], y[start:end]\n",
    "\n",
    "    def __fc_layer(self, layer_idx, prev_layer, units, activ=tf.nn.relu):\n",
    "        input_shape = prev_layer.get_shape()[1]\n",
    "\n",
    "        with tf.variable_scope(self.var_scope, reuse=tf.AUTO_REUSE):\n",
    "            weights = tf.get_variable(\n",
    "                'W_' + str(layer_idx), \n",
    "                dtype=tf.float32, \n",
    "                shape=[input_shape, units], \n",
    "                initializer=tf.truncated_normal_initializer(stddev=0.01)\n",
    "            )\n",
    "\n",
    "            bias = tf.get_variable(\n",
    "                'b_' + str(layer_idx), \n",
    "                dtype=tf.float32, \n",
    "                initializer=tf.constant(0., shape=[units], dtype=tf.float32)\n",
    "            )\n",
    "        \n",
    "        self.hidden_layers_W[layer_idx] = weights\n",
    "        self.hidden_layers_b[layer_idx] = bias\n",
    "        \n",
    "        layer = tf.matmul(prev_layer, weights) + bias\n",
    "        \n",
    "        if activ:\n",
    "            layer = tf.nn.dropout(layer, rate=self.dropout_rate)\n",
    "            layer = activ(layer)            \n",
    "        \n",
    "        return layer\n",
    "    \n",
    "\n",
    "#model = Lab2Model('scp_7', [784, 200, 10], lr=0.005, adaptive_lr=True, reg=0.1, dropout=0.5)\n",
    "#model.compile()\n",
    "#model.fit(X_tr, y_tr, batch_size=128, epochs=2)\n",
    "#model.predict(X_tr[0].reshape(1, -1)), y_tr[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 1.**\n",
    "\n",
    "Реализуйте полносвязную нейронную сеть с помощью библиотеки Tensor Flow. В качестве алгоритма оптимизации можно использовать, например, стохастический градиент (Stochastic Gradient Descent, SGD). Определите количество скрытых слоев от 1 до 5, количество нейронов в каждом из слоев до нескольких сотен, а также их функции активации (кусочно-линейная, сигмоидная, гиперболический тангенс и т.д.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T14:40:32.230440Z",
     "start_time": "2020-02-17T14:40:32.227638Z"
    }
   },
   "outputs": [],
   "source": [
    "network_config = [784, 150, 120, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T15:18:40.860289Z",
     "start_time": "2020-02-17T14:40:32.626793Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 380000,\t Valid size: 20000\n",
      "---------------------------------------------------------------\n",
      "Epochs: 150\t| Iterations: 1484\t| Batch: 256\n",
      "---------------------------------------------------------------\n",
      "Epoch: 1\n",
      "---------------------------------------------------------\n",
      "---------------------------------------------------------\n",
      "train loss: 2.30, train acc: 16.4%\n",
      "valid loss: 2.30, valid acc: 15.9%\n",
      "---------------------------------------------------------\n",
      "---------------------------------------------------------\n",
      "Epoch: 2\n"
     ]
    }
   ],
   "source": [
    "base_model = Lab2Model('base_8', network_config)\n",
    "base_model.compile()\n",
    "base_model.fit(train_X, train_y, batch_size=g_batch_size, epochs=g_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T15:20:03.298862Z",
     "start_time": "2020-02-17T15:20:03.010433Z"
    }
   },
   "outputs": [],
   "source": [
    "base_loss, base_acc = base_model.evaluate(test_X, test_y)\n",
    "base_loss, base_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 3.**\n",
    "\n",
    "Используйте регуляризацию и метод сброса нейронов (dropout) для борьбы с переобучением. Как улучшилось качество классификации?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T16:01:35.704565Z",
     "start_time": "2020-02-17T15:20:09.995826Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reg_drop_model = Lab2Model('reg_drop_9', network_config, reg=0.006, dropout=0.1)\n",
    "reg_drop_model.compile()\n",
    "reg_drop_model.fit(train_X, train_y, batch_size=g_batch_size, epochs=g_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T16:01:36.379609Z",
     "start_time": "2020-02-17T16:01:35.717201Z"
    }
   },
   "outputs": [],
   "source": [
    "reg_drop_loss, reg_drop_acc = reg_drop_model.evaluate(test_X, test_y)\n",
    "reg_drop_loss, reg_drop_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 4.**\n",
    "\n",
    "Воспользуйтесь динамически изменяемой скоростью обучения (learning rate). Наилучшая точность, достигнутая с помощью данной модели составляет 97.1%. Какую точность демонстрирует Ваша реализованная модель?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T16:40:54.828100Z",
     "start_time": "2020-02-17T16:01:36.385928Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reg_drop_adaptive_model = Lab2Model('reg_drop_9', network_config, reg=0.006, dropout=0.1, adaptive_lr=True)\n",
    "reg_drop_adaptive_model.compile()\n",
    "reg_drop_adaptive_model.fit(train_X, train_y, batch_size=g_batch_size, epochs=g_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T16:40:55.466968Z",
     "start_time": "2020-02-17T16:40:54.839376Z"
    }
   },
   "outputs": [],
   "source": [
    "reg_drop_adaptive_loss, reg_drop_adaptive_acc = reg_drop_adaptive_model.evaluate(test_X, test_y)\n",
    "reg_drop_adaptive_loss, reg_drop_adaptive_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:27:01.345962Z",
     "start_time": "2020-02-17T16:48:52.410166Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reg_model = Lab2Model('reg_9', network_config,adaptive_lr=True)\n",
    "reg_model.compile()\n",
    "reg_model.fit(train_X, train_y, batch_size=g_batch_size, epochs=g_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:27:02.048846Z",
     "start_time": "2020-02-17T17:27:01.370455Z"
    }
   },
   "outputs": [],
   "source": [
    "reg_loss, reg_acc = reg_model.evaluate(test_X, test_y)\n",
    "reg_loss, reg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:28:21.866958Z",
     "start_time": "2020-02-17T17:28:21.855296Z"
    }
   },
   "outputs": [],
   "source": [
    "def calc_lr(current_iteration, iterations_count):        \n",
    "        adaptive_rate = 200\n",
    "\n",
    "        return 0.003 * adaptive_rate ** (1-(current_iteration / iterations_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:29:10.477085Z",
     "start_time": "2020-02-17T17:29:10.471736Z"
    }
   },
   "outputs": [],
   "source": [
    "lr_list = list()\n",
    "for e in range(g_epochs):\n",
    "    lr_list.append(calc_lr(e,g_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:57:38.163397Z",
     "start_time": "2020-02-17T17:57:37.488328Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 5))\n",
    "plt.plot(lr_list)\n",
    "plt.title('Adaptive Learning rate')\n",
    "plt.ylabel('Learning rate')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T17:46:05.153074Z",
     "start_time": "2020-02-17T17:46:05.150210Z"
    }
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Base': base_model,\n",
    "    'Drop + Reg': reg_drop_model,\n",
    "    'Drop + Reg + A_lr': reg_drop_adaptive_model,\n",
    "    'Adaptive lr': reg_model,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T18:21:48.754279Z",
     "start_time": "2020-02-17T18:21:48.748266Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_curves(title, data, y_title, x_title='Epoch', legend=[], x_labels_offset = 0):\n",
    "    fig = plt.figure(figsize=(8, 4))\n",
    "        \n",
    "    for row in data:\n",
    "        x = range(x_labels_offset, len(row))\n",
    "        plt.plot(x, row[x_labels_offset:])\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.ylabel(y_title)\n",
    "    plt.xlabel(x_title)\n",
    "    plt.grid(True)\n",
    "    plt.legend(legend, loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T18:21:53.156486Z",
     "start_time": "2020-02-17T18:21:50.293230Z"
    }
   },
   "outputs": [],
   "source": [
    "for name in models:\n",
    "    offset = 50\n",
    "    m = models[name]\n",
    "    \n",
    "    plot_curves(f'{name} - Accuracy', [m.history['acc_train'], m.history['acc_valid']], 'Accuracy', 'Epoch', ['Train', 'Valid'])\n",
    "    plot_curves(f'{name} - Accuracy', [m.history['acc_train'], m.history['acc_valid']], 'Accuracy', 'Epoch', ['Train', 'Valid'], x_labels_offset=offset)\n",
    "    plot_curves(f'{name} - Loss', [m.history['loss_train'], m.history['loss_valid']], 'Loss', 'Epoch', ['Train', 'Valid'])\n",
    "    plot_curves(f'{name} - Loss', [m.history['loss_train'], m.history['loss_valid']], 'Loss', 'Epoch', ['Train', 'Valid'], x_labels_offset=offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T18:18:10.305299Z",
     "start_time": "2020-02-17T18:18:10.301562Z"
    }
   },
   "outputs": [],
   "source": [
    "loses = list()\n",
    "acc = list()\n",
    "labels = list()\n",
    "for name in models:\n",
    "    m = models[name]\n",
    "    labels.append(name)\n",
    "    loses.append(m.history['loss_valid'])\n",
    "    acc.append(m.history['acc_valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T18:22:44.452125Z",
     "start_time": "2020-02-17T18:22:44.291278Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_curves(f'Accuracy', acc, 'Accuracy', 'Epoch', labels, x_labels_offset=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T18:25:04.842066Z",
     "start_time": "2020-02-17T18:25:04.677813Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_curves(f'Loss', loses, 'Loss', 'Epoch', labels, x_labels_offset=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T18:28:21.132397Z",
     "start_time": "2020-02-17T18:28:20.096069Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = list()\n",
    "test_acc = list()\n",
    "for name in models:\n",
    "    m = models[name]\n",
    "    labels.append(name)\n",
    "    _, t_acc = m.evaluate(test_X, test_y)\n",
    "    test_acc.append(t_acc)\n",
    "\n",
    "labels, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T18:40:56.832366Z",
     "start_time": "2020-02-17T18:40:56.720427Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.bar(labels,test_acc, color=['Blue', 'Orange', 'Green', 'Red'])\n",
    "plt.ylim(0.916, 0.925)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

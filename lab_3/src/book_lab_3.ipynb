{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 1.**\n",
    "\n",
    "Реализуйте нейронную сеть с двумя сверточными слоями, и одним полносвязным с нейронами с кусочно-линейной функцией активации. Какова точность построенное модели?\n",
    "\n",
    "**Задание 2.**\n",
    "\n",
    "Замените один из сверточных слоев на слой, реализующий операцию пулинга (Pooling) с функцией максимума или среднего. Как это повлияло на точность классификатора?\n",
    "\n",
    "**Задание 3.**\n",
    "\n",
    "Реализуйте классическую архитектуру сверточных сетей LeNet-5 (http://yann.lecun.com/exdb/lenet/).\n",
    "\n",
    "**Задание 4.**\n",
    "\n",
    "Сравните максимальные точности моделей, построенных в лабораторных работах 1-3. Как можно объяснить полученные различия?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T21:49:39.252814Z",
     "start_time": "2020-03-09T21:49:39.249596Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T21:49:39.761239Z",
     "start_time": "2020-03-09T21:49:39.757826Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T20:41:34.376213Z",
     "start_time": "2020-03-09T20:41:34.371573Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T20:41:34.380836Z",
     "start_time": "2020-03-09T20:41:34.377860Z"
    }
   },
   "outputs": [],
   "source": [
    "large_dataset_path = '../../lab_1/src/notMNIST_large_clean.mat'\n",
    "small_dataset_path = '../../lab_1/src/notMNIST_small_uniq.mat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T20:41:34.392489Z",
     "start_time": "2020-03-09T20:41:34.382787Z"
    }
   },
   "outputs": [],
   "source": [
    "chars = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\n",
    "def prepare_dataset(dataset, records=None):\n",
    "    data = list()\n",
    "    labels = list()\n",
    "\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(chars)\n",
    "    \n",
    "    if records:\n",
    "        one = int(records / len(chars))\n",
    "        #check\n",
    "        for ch in chars:\n",
    "            ch_len = len(dataset[ch])\n",
    "            assert ch_len >= one, f'\"{ch}\" has {ch_len} items but required {one}'\n",
    "        #print(one)\n",
    "        for ch in chars:\n",
    "            indexes = np.random.choice(len(dataset[ch]), one)\n",
    "            picked_elements = dataset[ch][indexes] / 255\n",
    "            data.extend(picked_elements)\n",
    "            labels.extend(np.tile(lb.transform([ch])[0], (one, 1)))\n",
    "    else:\n",
    "        for ch in chars:\n",
    "            data.extend(dataset[ch]/255)\n",
    "            labels.extend(np.tile(lb.transform([ch])[0], (len(dataset[ch]), 1)))\n",
    "    \n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return resample(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T20:41:34.400412Z",
     "start_time": "2020-03-09T20:41:34.394123Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_curves(title, data, y_title, x_title='Epoch', legend=[], x_labels_offset = 0):\n",
    "    fig = plt.figure(figsize=(8, 4))\n",
    "        \n",
    "    for row in data:\n",
    "        x = range(x_labels_offset, len(row))\n",
    "        plt.plot(x, row[x_labels_offset:])\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.ylabel(y_title)\n",
    "    plt.xlabel(x_title)\n",
    "    plt.grid(True)\n",
    "    plt.legend(legend, loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T20:41:34.467825Z",
     "start_time": "2020-03-09T20:41:34.402816Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18238, 784)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_data = scipy.io.loadmat(small_dataset_path)\n",
    "test_X, test_y = prepare_dataset(small_data)\n",
    "test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T21:36:43.915662Z",
     "start_time": "2020-03-09T21:36:43.272317Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 784)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_data = scipy.io.loadmat(large_dataset_path)\n",
    "train_X, train_y = prepare_dataset(large_data, 400_000)\n",
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T20:41:35.144765Z",
     "start_time": "2020-03-09T20:41:35.140262Z"
    }
   },
   "outputs": [],
   "source": [
    "def rand_str(str_len=20):\n",
    "    return ''.join(random.choices(string.ascii_uppercase + string.digits, k=str_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T21:56:40.854985Z",
     "start_time": "2020-03-09T21:56:40.819484Z"
    }
   },
   "outputs": [],
   "source": [
    "class BaseModel():\n",
    "    def __init__(self):\n",
    "        self.init_basic_params()\n",
    "        \n",
    "        self.compile()\n",
    "        \n",
    "        self.tf_writer.add_graph(self.session.graph)\n",
    "    \n",
    "    def init_basic_params(self):\n",
    "        tf.reset_default_graph()\n",
    "        self.logs_path = './tf_board/' + self.__class__.__name__\n",
    "        self.var_scope = rand_str()\n",
    "        self.print_separator = '-' * 65\n",
    "        self.session = None\n",
    "        self.dropout_rate_tf = tf.placeholder(tf.float32)\n",
    "        \n",
    "        self.input_size = 784\n",
    "        self.output_size = 10\n",
    "        \n",
    "        self.dropout_rate = 0.1\n",
    "        \n",
    "        self.start_learning_rate = 0.01\n",
    "        \n",
    "        self.tf_writer = tf.summary.FileWriter(self.logs_path)\n",
    "    \n",
    "    def __del__(self): \n",
    "        print('object del')\n",
    "        if self.session:\n",
    "            tf.reset_default_graph()\n",
    "            self.session.close()\n",
    "    \n",
    "    def reset_internal_params(self):\n",
    "        self.hidden_layers = {}\n",
    "        self.hidden_layers_W = {}\n",
    "        self.hidden_layers_b = {}\n",
    "        self.history = {\n",
    "            'acc_train': [],\n",
    "            'acc_valid': [],\n",
    "            'loss_train': [],\n",
    "            'loss_valid': []\n",
    "        }\n",
    "        \n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        self.decay_steps = tf.Variable(100000, trainable=False)\n",
    "    \n",
    "    def get_W(self, layer_id, shape):\n",
    "        #with tf.variable_scope(self.var_scope, reuse=tf.AUTO_REUSE):\n",
    "        W = tf.Variable(tf.truncated_normal(shape, stddev=0.1, mean=0), name=f'W_{layer_id}')\n",
    "        self.hidden_layers_W[layer_id] = W\n",
    "\n",
    "        return self.hidden_layers_W[layer_id]\n",
    "\n",
    "    def get_b(self, layer_id, shape):\n",
    "        #with tf.variable_scope(self.var_scope, reuse=tf.AUTO_REUSE):\n",
    "        self.hidden_layers_b[layer_id] = tf.Variable(tf.zeros(shape), name=f'b_{layer_id}')\n",
    "        \n",
    "        return self.hidden_layers_b[layer_id]\n",
    "        \n",
    "    def get_dense_layer(self, layer_id, prev_layer, units_count, activ=tf.nn.relu):\n",
    "        input_size = prev_layer.get_shape().as_list()[1]\n",
    "        \n",
    "        W = self.get_W(layer_id, [input_size, units_count])\n",
    "        b = self.get_b(layer_id, [units_count])\n",
    "        \n",
    "        layer = tf.matmul(prev_layer, W) + b\n",
    "        \n",
    "        if activ:\n",
    "            layer = activ(layer, name=f'Lay_Dense_{layer_id}')\n",
    "        \n",
    "        return layer\n",
    "    \n",
    "    def get_conv2_layer(self, layer_id, prev_layer, kernel_size, output_channels, strides=1, padding='SAME', activ=tf.nn.relu):\n",
    "        input_channels = prev_layer.get_shape().as_list()[3]\n",
    "        \n",
    "        # 5x5 conv, 1 input, 32 outputs\n",
    "        # [filter_height, filter_width, in_channels, out_channels]\n",
    "        filter_shape = [kernel_size[0], kernel_size[1], input_channels, output_channels]\n",
    "\n",
    "        W = self.get_W(layer_id, filter_shape)\n",
    "        b = self.get_b(layer_id, [output_channels])\n",
    "        \n",
    "        layer = tf.nn.conv2d(prev_layer, W, [1, strides, strides, 1], padding) + b\n",
    "        \n",
    "        if activ:\n",
    "            layer = activ(layer, name=f'Lay_Conv2d_{layer_id}')  \n",
    "        \n",
    "        return layer\n",
    "    \n",
    "    def next_batch(self, x, y, batch_size, iteration):\n",
    "        start = iteration * batch_size\n",
    "        end = (iteration + 1) * batch_size\n",
    "        \n",
    "        return x[start:end], y[start:end]\n",
    "    \n",
    "    def flatten(self, input):\n",
    "        shape = input.get_shape().as_list()\n",
    "        shape = np.array(shape)\n",
    "        size = shape[shape != None].prod()\n",
    "        \n",
    "        return tf.reshape(input, [-1, size], name='Flatten')\n",
    "    \n",
    "    def get_max_pooling(self, layer_id, input, ksize, stride=1, padding='SAME'):\n",
    "        return tf.nn.max_pool(input, [1, ksize, ksize, 1], [1, stride, stride, 1], padding, name=f'max_pool_{layer_id}')\n",
    "\n",
    "    def get_avg_pooling(self, layer_id, input, ksize, stride=1, padding='SAME'):\n",
    "        return tf.nn.avg_pool(input, [1, ksize, ksize, 1], [1, stride, stride, 1], padding, name=f'avg_pool_{layer_id}')\n",
    "    \n",
    "    def pre_compile():\n",
    "        print('precompile')\n",
    "    \n",
    "    def compile(self):\n",
    "        \n",
    "        self.lr = tf.placeholder(tf.float32, shape=[])\n",
    "        \n",
    "        self.pre_compile()\n",
    "        \n",
    "        self.prediction = tf.nn.softmax(self.layer_output, name='Output')\n",
    "        self.accuracy = tf.reduce_mean(\n",
    "            tf.cast(\n",
    "                tf.equal(tf.argmax(self.prediction, 1), tf.argmax(self.expected_output, 1)),\n",
    "                tf.float32,\n",
    "            ),\n",
    "            name='Accuracy'\n",
    "        )\n",
    "                \n",
    "        self.session = tf.Session()\n",
    "        self.vars = tf.global_variables_initializer()\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def fit(self, x=None, y=None, batch_size=64, epochs=1):\n",
    "        \n",
    "        valid_size = 0.3\n",
    "        \n",
    "        if len(y) * valid_size > 20_000:\n",
    "            valid_size = 20_000\n",
    "        \n",
    "        x_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size=valid_size, random_state=50)\n",
    "        \n",
    "        print(f'Train size: {len(y_train)},\\t Valid size: {len(y_valid)}')\n",
    "        \n",
    "        iterations = int(len(y_train) / batch_size)\n",
    "        \n",
    "        display_info = int(iterations / 2)\n",
    "        \n",
    "        print(self.print_separator)\n",
    "        print(f'Epochs: {epochs}\\t| Iterations: {iterations}\\t| Batch: {batch_size}')\n",
    "        print(self.print_separator)\n",
    "        \n",
    "        self.session.run(self.decay_steps.assign(iterations))\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            x_train_epoch, y_train_epoch = resample(x_train, y_train)\n",
    "            \n",
    "            epoch_train_acc = 0\n",
    "            epoch_train_loss = 0\n",
    "            \n",
    "            for iteration in range(iterations):\n",
    "                x_batch, y_batch = self.next_batch(x_train_epoch, y_train_epoch, batch_size, iteration)\n",
    "\n",
    "                feed_data = { \n",
    "                    self.input: x_batch, \n",
    "                    self.expected_output: y_batch,\n",
    "                    self.dropout_rate_tf: self.dropout_rate\n",
    "                }\n",
    "                \n",
    "                _, loss_train, acc_train = self.session.run([self.optimizer, self.loss, self.accuracy], feed_dict=feed_data)\n",
    "                \n",
    "                epoch_train_acc += acc_train / iterations\n",
    "                epoch_train_loss += loss_train / iterations\n",
    "            \n",
    "            #feed_data_train = { self.input: x_batch, self.expected_output: y_batch, self.dropout_rate_tf: 0}\n",
    "            #loss_train, acc_train = self.session.run([self.loss, self.accuracy], feed_dict=feed_data_train)\n",
    "            \n",
    "            # print(self.session.run([self.global_step, self.decay_steps, self.lr]))\n",
    "            \n",
    "            self.history['acc_train'].append(epoch_train_acc)\n",
    "            self.history['loss_train'].append(epoch_train_loss)\n",
    "\n",
    "            feed_data_valid = { self.input: x_valid, self.expected_output: y_valid, self.dropout_rate_tf: 0}\n",
    "            loss_valid, acc_valid = self.session.run([self.loss, self.accuracy], feed_dict=feed_data_valid)\n",
    "\n",
    "            self.history['acc_valid'].append(acc_valid)\n",
    "            self.history['loss_valid'].append(loss_valid)\n",
    "\n",
    "            print(f'Epoch {epoch + 1}: loss - Tr[{loss_train:.3f}] Va[{loss_valid:.3f}] \\t acc - Tr[{acc_train:.01%}] Va[{acc_valid:.01%}]')\n",
    "            print(self.print_separator)\n",
    "            \n",
    "    def evaluate(self, x=None, y=None):\n",
    "        feed_data = { self.input: x, self.expected_output: y, self.dropout_rate_tf: 0}\n",
    "        return self.session.run([self.loss, self.accuracy], feed_dict=feed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T21:56:43.525119Z",
     "start_time": "2020-03-09T21:56:43.515671Z"
    }
   },
   "outputs": [],
   "source": [
    "class Model1CCD(BaseModel):\n",
    "    def __init__(self):\n",
    "        BaseModel.__init__(self)\n",
    "\n",
    "    def pre_compile(self):\n",
    "        self.reset_internal_params()\n",
    "        \n",
    "        self.input = tf.placeholder(tf.float32, shape=[None, self.input_size], name=\"Input\")\n",
    "        self.expected_output = tf.placeholder(tf.float32, shape=[None, self.output_size], name=\"Y_actual\")\n",
    "        \n",
    "        # [batch, in_height, in_width, in_channels]\n",
    "        layer_1_pre = tf.reshape(self.input, [-1, 28, 28, 1])\n",
    "        \n",
    "        layer_1 = self.get_conv2_layer(1, layer_1_pre, [4, 4], 20, strides=1)\n",
    "        \n",
    "        layer_2 = self.get_conv2_layer(2, layer_1, [4, 4], 20, strides=1)\n",
    "                    \n",
    "        layer_3_pre = self.flatten(layer_2)\n",
    "        \n",
    "        layer_3 = self.get_dense_layer(3, layer_3_pre, 120)\n",
    "        layer_3_do = tf.nn.dropout(layer_3, rate=self.dropout_rate_tf)\n",
    "        \n",
    "        self.layer_output = self.get_dense_layer(4, layer_3_do, 10, activ=None)\n",
    "        \n",
    "        self.loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.expected_output, logits=self.layer_output),\n",
    "            name='Loss'\n",
    "        )\n",
    "        \n",
    "        self.lr = tf.compat.v1.train.exponential_decay(self.start_learning_rate, self.global_step, self.decay_steps, 0.96)\n",
    "        \n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(self.lr).minimize(self.loss, global_step=self.global_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T21:57:49.733913Z",
     "start_time": "2020-03-09T21:57:49.725153Z"
    }
   },
   "outputs": [],
   "source": [
    "class Model2CPD(BaseModel):\n",
    "    def __init__(self):\n",
    "        BaseModel.__init__(self)\n",
    "\n",
    "    def pre_compile(self):\n",
    "        self.reset_internal_params()\n",
    "        \n",
    "        self.input = tf.placeholder(tf.float32, shape=[None, self.input_size], name=\"Input\")\n",
    "        self.expected_output = tf.placeholder(tf.float32, shape=[None, self.output_size], name=\"Y_actual\")\n",
    "        \n",
    "        # [batch, in_height, in_width, in_channels]\n",
    "        layer_1_pre = tf.reshape(self.input, [-1, 28, 28, 1])\n",
    "        \n",
    "        layer_1 = self.get_conv2_layer(1, layer_1_pre, [4, 4], 20, strides=2)\n",
    "        \n",
    "        layer_2 = self.get_max_pooling(2, layer_1, 3)\n",
    "                    \n",
    "        layer_3_pre = self.flatten(layer_2)\n",
    "        \n",
    "        layer_3 = self.get_dense_layer(3, layer_3_pre, 120)\n",
    "        layer_3_do = tf.nn.dropout(layer_3, rate=self.dropout_rate_tf)\n",
    "        \n",
    "        self.layer_output = self.get_dense_layer(4, layer_3_do, 10, activ=None)\n",
    "        \n",
    "        self.loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.expected_output, logits=self.layer_output),\n",
    "            name='Loss'\n",
    "        )\n",
    "        \n",
    "        self.lr = tf.compat.v1.train.exponential_decay(self.start_learning_rate, self.global_step, self.decay_steps, 0.96)\n",
    "        \n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(self.lr).minimize(self.loss, global_step=self.global_step)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T22:05:42.463004Z",
     "start_time": "2020-03-09T22:05:42.447451Z"
    }
   },
   "outputs": [],
   "source": [
    "class LeNet5(BaseModel):\n",
    "    def __init__(self):\n",
    "        BaseModel.__init__(self)\n",
    "    \n",
    "    def init_basic_params(self):\n",
    "        super().init_basic_params()\n",
    "        self.input_size = [32, 32, 1]\n",
    "    \n",
    "    def pad_x(self, x):\n",
    "        temp = x.reshape(-1, 28, 28, 1)\n",
    "        return np.pad(temp, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "    \n",
    "    def fit(self, x=None, y=None, batch_size=64, epochs=1):\n",
    "        x_pad = self.pad_x(x)\n",
    "        super().fit(x=x_pad, y=y, batch_size=batch_size, epochs=epochs) \n",
    "        \n",
    "    def evaluate(self, x=None, y=None):\n",
    "        x_pad = self.pad_x(x)\n",
    "        return super().evaluate(x=x_pad, y=y) \n",
    "        \n",
    "    def pre_compile(self):\n",
    "        self.reset_internal_params()\n",
    "\n",
    "        self.input = tf.placeholder(tf.float32, shape=[None, *self.input_size], name=\"Input\")\n",
    "        self.expected_output = tf.placeholder(tf.float32, shape=[None, self.output_size], name=\"Y_actual\")\n",
    "   \n",
    "        # Conv. out - 6, kernel - 5x5, stride = 1\n",
    "        layer_1 = self.get_conv2_layer(1, self.input, [5, 5], 6, strides=1, padding='VALID', activ=tf.nn.tanh)\n",
    "        \n",
    "        # avg pool. filter = 2x2, stride = 2\n",
    "        layer_2 = self.get_avg_pooling(2, layer_1, 2, stride=2, padding='VALID')\n",
    "        \n",
    "        # Conv. out - 16, kernel - 5x5, stride = 1\n",
    "        layer_3 = self.get_conv2_layer(3, layer_2, [5, 5], 16, strides=1, padding='VALID', activ=tf.nn.tanh)\n",
    "        \n",
    "        # avg pool. filter = 2x2, stride = 2\n",
    "        layer_4 = self.get_avg_pooling(4, layer_3, 2, stride=2, padding='VALID')\n",
    "        \n",
    "        # Conv. out - 120, kernel - 5x5, stride = 1\n",
    "        layer_5 = self.get_conv2_layer(5, layer_4, [5, 5], 120, strides=1, padding='VALID', activ=tf.nn.tanh)\n",
    "        \n",
    "        # Dense - 84\n",
    "        layer_6_pre = self.flatten(layer_5)\n",
    "        \n",
    "        layer_6 = self.get_dense_layer(6, layer_6_pre, 84, activ=tf.nn.tanh)\n",
    "        \n",
    "        self.layer_output = self.get_dense_layer(4, layer_6, 10, activ=None)\n",
    "        \n",
    "        self.loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.expected_output, logits=self.layer_output),\n",
    "            name='Loss'\n",
    "        )\n",
    "        \n",
    "        self.lr = tf.compat.v1.train.exponential_decay(self.start_learning_rate, self.global_step, self.decay_steps, 0.96)\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(self.lr).minimize(self.loss, global_step=self.global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T21:38:27.193014Z",
     "start_time": "2020-03-09T21:38:27.188514Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T22:05:45.161136Z",
     "start_time": "2020-03-09T22:05:44.429702Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object del\n",
      "object del\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    'conv + conv + dense': Model1CCD(),\n",
    "    'conv + max pool + dense': Model2CPD(),\n",
    "    'LeNet5': LeNet5(),\n",
    "}\n",
    "\n",
    "# loss, accuracy\n",
    "evaluation = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T22:03:57.173773Z",
     "start_time": "2020-03-09T22:03:57.169485Z"
    }
   },
   "outputs": [],
   "source": [
    "train_epochs = 10\n",
    "train_batch_size = 128\n",
    "\n",
    "def train_and_evaluate(name):\n",
    "    print('train_and_evaluate')\n",
    "    print('-------------------')\n",
    "    print(f'Model: {name}')\n",
    "    print('-------------------')\n",
    "    \n",
    "    m = models[name]\n",
    "    m.fit(train_X, train_y, batch_size=train_batch_size, epochs=train_epochs)\n",
    "    evaluation[name] = m.evaluate(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T22:08:13.815275Z",
     "start_time": "2020-03-09T22:07:28.766293Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_and_evaluate\n",
      "-------------------\n",
      "Model: conv + conv + dense\n",
      "-------------------\n",
      "Train size: 380000,\t Valid size: 20000\n",
      "-----------------------------------------------------------------\n",
      "Epochs: 40\t| Iterations: 2968\t| Batch: 128\n",
      "-----------------------------------------------------------------\n",
      "Epoch 1: loss - Tr[0.412] Va[0.406] \t acc - Tr[87.5%] Va[87.8%]\n",
      "-----------------------------------------------------------------\n",
      "Epoch 2: loss - Tr[0.536] Va[0.402] \t acc - Tr[82.8%] Va[87.9%]\n",
      "-----------------------------------------------------------------\n",
      "Epoch 3: loss - Tr[0.394] Va[0.392] \t acc - Tr[88.3%] Va[88.2%]\n",
      "-----------------------------------------------------------------\n",
      "Epoch 4: loss - Tr[0.434] Va[0.387] \t acc - Tr[88.3%] Va[88.3%]\n",
      "-----------------------------------------------------------------\n",
      "Epoch 5: loss - Tr[0.366] Va[0.384] \t acc - Tr[89.1%] Va[88.5%]\n",
      "-----------------------------------------------------------------\n",
      "Epoch 6: loss - Tr[0.346] Va[0.379] \t acc - Tr[88.3%] Va[88.7%]\n",
      "-----------------------------------------------------------------\n",
      "Epoch 7: loss - Tr[0.225] Va[0.372] \t acc - Tr[94.5%] Va[88.9%]\n",
      "-----------------------------------------------------------------\n",
      "Epoch 8: loss - Tr[0.462] Va[0.367] \t acc - Tr[87.5%] Va[89.0%]\n",
      "-----------------------------------------------------------------\n",
      "Epoch 9: loss - Tr[0.392] Va[0.364] \t acc - Tr[85.9%] Va[89.1%]\n",
      "-----------------------------------------------------------------\n",
      "Epoch 10: loss - Tr[0.326] Va[0.361] \t acc - Tr[91.4%] Va[89.1%]\n",
      "-----------------------------------------------------------------\n",
      "Epoch 11: loss - Tr[0.281] Va[0.358] \t acc - Tr[92.2%] Va[89.2%]\n",
      "-----------------------------------------------------------------\n",
      "Epoch 12: loss - Tr[0.390] Va[0.354] \t acc - Tr[89.8%] Va[89.4%]\n",
      "-----------------------------------------------------------------\n",
      "Epoch 13: loss - Tr[0.389] Va[0.352] \t acc - Tr[90.6%] Va[89.5%]\n",
      "-----------------------------------------------------------------\n",
      "Epoch 14: loss - Tr[0.302] Va[0.350] \t acc - Tr[93.0%] Va[89.4%]\n",
      "-----------------------------------------------------------------\n",
      "Epoch 15: loss - Tr[0.255] Va[0.349] \t acc - Tr[90.6%] Va[89.5%]\n",
      "-----------------------------------------------------------------\n",
      "Epoch 16: loss - Tr[0.204] Va[0.345] \t acc - Tr[93.8%] Va[89.6%]\n",
      "-----------------------------------------------------------------\n",
      "Epoch 17: loss - Tr[0.442] Va[0.344] \t acc - Tr[88.3%] Va[89.6%]\n",
      "-----------------------------------------------------------------\n",
      "Epoch 18: loss - Tr[0.241] Va[0.342] \t acc - Tr[92.2%] Va[89.7%]\n",
      "-----------------------------------------------------------------\n",
      "Epoch 19: loss - Tr[0.374] Va[0.340] \t acc - Tr[86.7%] Va[89.7%]\n",
      "-----------------------------------------------------------------\n",
      "Epoch 20: loss - Tr[0.499] Va[0.338] \t acc - Tr[85.9%] Va[89.8%]\n",
      "-----------------------------------------------------------------\n",
      "Epoch 21: loss - Tr[0.371] Va[0.335] \t acc - Tr[89.1%] Va[89.9%]\n",
      "-----------------------------------------------------------------\n",
      "Epoch 22: loss - Tr[0.342] Va[0.335] \t acc - Tr[89.8%] Va[90.0%]\n",
      "-----------------------------------------------------------------\n",
      "Epoch 23: loss - Tr[0.285] Va[0.334] \t acc - Tr[89.1%] Va[89.9%]\n",
      "-----------------------------------------------------------------\n",
      "Epoch 24: loss - Tr[0.215] Va[0.333] \t acc - Tr[90.6%] Va[90.0%]\n",
      "-----------------------------------------------------------------\n",
      "Epoch 25: loss - Tr[0.290] Va[0.331] \t acc - Tr[89.1%] Va[90.0%]\n",
      "-----------------------------------------------------------------\n",
      "Epoch 26: loss - Tr[0.351] Va[0.331] \t acc - Tr[88.3%] Va[89.9%]\n",
      "-----------------------------------------------------------------\n",
      "Epoch 27: loss - Tr[0.254] Va[0.330] \t acc - Tr[93.0%] Va[90.0%]\n",
      "-----------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-702a80503c5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'conv + conv + dense'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-45-e0cc60625220>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mevaluation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-dda76c00abeb>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 }\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0mepoch_train_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0macc_train\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_and_evaluate('conv + conv + dense')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T22:08:20.368035Z",
     "start_time": "2020-03-09T22:08:13.817784Z"
    }
   },
   "outputs": [],
   "source": [
    "train_and_evaluate('conv + max pool + dense')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T22:06:02.305226Z",
     "start_time": "2020-03-09T22:05:49.795859Z"
    }
   },
   "outputs": [],
   "source": [
    "train_and_evaluate('LeNet5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T22:08:20.378691Z",
     "start_time": "2020-03-09T22:08:20.370566Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T22:17:31.491065Z",
     "start_time": "2020-03-09T22:17:30.015090Z"
    }
   },
   "outputs": [],
   "source": [
    "for name in models:\n",
    "    offset = 50\n",
    "    m = models[name]\n",
    "    \n",
    "    plot_curves(f'{name} - Accuracy', [m.history['acc_train'], m.history['acc_valid']], 'Accuracy', 'Epoch', ['Train', 'Valid'])\n",
    "    # plot_curves(f'{name} - Accuracy', [m.history['acc_train'], m.history['acc_valid']], 'Accuracy', 'Epoch', ['Train', 'Valid'], x_labels_offset=offset)\n",
    "    plot_curves(f'{name} - Loss', [m.history['loss_train'], m.history['loss_valid']], 'Loss', 'Epoch', ['Train', 'Valid'])\n",
    "    # plot_curves(f'{name} - Loss', [m.history['loss_train'], m.history['loss_valid']], 'Loss', 'Epoch', ['Train', 'Valid'], x_labels_offset=offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

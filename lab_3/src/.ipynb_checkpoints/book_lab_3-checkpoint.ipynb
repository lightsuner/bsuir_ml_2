{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 1.**\n",
    "\n",
    "Реализуйте нейронную сеть с двумя сверточными слоями, и одним полносвязным с нейронами с кусочно-линейной функцией активации. Какова точность построенное модели?\n",
    "\n",
    "**Задание 2.**\n",
    "\n",
    "Замените один из сверточных слоев на слой, реализующий операцию пулинга (Pooling) с функцией максимума или среднего. Как это повлияло на точность классификатора?\n",
    "\n",
    "**Задание 3.**\n",
    "\n",
    "Реализуйте классическую архитектуру сверточных сетей LeNet-5 (http://yann.lecun.com/exdb/lenet/).\n",
    "\n",
    "**Задание 4.**\n",
    "\n",
    "Сравните максимальные точности моделей, построенных в лабораторных работах 1-3. Как можно объяснить полученные различия?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T19:51:53.801047Z",
     "start_time": "2020-03-08T19:51:53.797964Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T12:37:58.130068Z",
     "start_time": "2020-03-08T12:37:58.126637Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T12:10:10.378259Z",
     "start_time": "2020-03-08T12:10:10.363581Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T12:10:10.389573Z",
     "start_time": "2020-03-08T12:10:10.382683Z"
    }
   },
   "outputs": [],
   "source": [
    "large_dataset_path = '../../lab_1/src/notMNIST_large_clean.mat'\n",
    "small_dataset_path = '../../lab_1/src/notMNIST_small_uniq.mat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T12:10:10.420358Z",
     "start_time": "2020-03-08T12:10:10.394083Z"
    }
   },
   "outputs": [],
   "source": [
    "chars = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\n",
    "def prepare_dataset(dataset, records=None):\n",
    "    data = list()\n",
    "    labels = list()\n",
    "\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(chars)\n",
    "    \n",
    "    if records:\n",
    "        one = int(records / len(chars))\n",
    "        #check\n",
    "        for ch in chars:\n",
    "            ch_len = len(dataset[ch])\n",
    "            assert ch_len >= one, f'\"{ch}\" has {ch_len} items but required {one}'\n",
    "        #print(one)\n",
    "        for ch in chars:\n",
    "            indexes = np.random.choice(len(dataset[ch]), one)\n",
    "            picked_elements = dataset[ch][indexes] / 255\n",
    "            data.extend(picked_elements)\n",
    "            labels.extend(np.tile(lb.transform([ch])[0], (one, 1)))\n",
    "    else:\n",
    "        for ch in chars:\n",
    "            data.extend(dataset[ch]/255)\n",
    "            labels.extend(np.tile(lb.transform([ch])[0], (len(dataset[ch]), 1)))\n",
    "    \n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return resample(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T12:10:10.586101Z",
     "start_time": "2020-03-08T12:10:10.427377Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 784)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_data = scipy.io.loadmat(small_dataset_path)\n",
    "test_X, test_y = prepare_dataset(small_data, 2000)\n",
    "test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T20:28:38.867072Z",
     "start_time": "2020-03-08T20:28:38.109361Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_data = scipy.io.loadmat(large_dataset_path)\n",
    "train_X, train_y = prepare_dataset(large_data, 10000)\n",
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T12:37:49.851183Z",
     "start_time": "2020-03-08T12:37:49.848141Z"
    }
   },
   "outputs": [],
   "source": [
    "def rand_str(str_len=20):\n",
    "    return ''.join(random.choices(string.ascii_uppercase + string.digits, k=str_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T22:47:13.455025Z",
     "start_time": "2020-03-08T22:47:13.404346Z"
    }
   },
   "outputs": [],
   "source": [
    "class BaseModel():\n",
    "    def __init__(self):\n",
    "        self.init_basic_params()\n",
    "        \n",
    "        self.compile()\n",
    "        \n",
    "        self.tf_writer.add_graph(self.session.graph)\n",
    "    \n",
    "    def init_basic_params(self):\n",
    "        tf.reset_default_graph()\n",
    "        self.logs_path = './tf_board/' + self.__class__.__name__\n",
    "        self.var_scope = rand_str()\n",
    "        self.print_separator = '-' * 65\n",
    "        self.session = None\n",
    "        self.dropout_rate_tf = tf.placeholder(tf.float32)\n",
    "        \n",
    "        self.input_size = 784\n",
    "        self.output_size = 10\n",
    "        \n",
    "        self.dropout_rate = 0.1\n",
    "        \n",
    "        self.start_learning_rate = 0.1\n",
    "        \n",
    "        self.tf_writer = tf.summary.FileWriter(self.logs_path)\n",
    "    \n",
    "    def __del__(self): \n",
    "        print('object del')\n",
    "        if self.session:\n",
    "            tf.reset_default_graph()\n",
    "            self.session.close()\n",
    "    \n",
    "    def reset_internal_params(self):\n",
    "        self.hidden_layers = {}\n",
    "        self.hidden_layers_W = {}\n",
    "        self.hidden_layers_b = {}\n",
    "        self.history = {\n",
    "            'acc_train': [],\n",
    "            'acc_valid': [],\n",
    "            'loss_train': [],\n",
    "            'loss_valid': []\n",
    "        }\n",
    "        \n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        self.decay_steps = tf.Variable(100000, trainable=False)\n",
    "    \n",
    "    def get_W(self, layer_id, shape):\n",
    "        #with tf.variable_scope(self.var_scope, reuse=tf.AUTO_REUSE):\n",
    "        W = tf.Variable(tf.truncated_normal(shape, stddev=0.01), name=f'W_{layer_id}')\n",
    "        self.hidden_layers_W[layer_id] = W\n",
    "\n",
    "        return self.hidden_layers_W[layer_id]\n",
    "\n",
    "    def get_b(self, layer_id, shape):\n",
    "        #with tf.variable_scope(self.var_scope, reuse=tf.AUTO_REUSE):\n",
    "        self.hidden_layers_b[layer_id] = tf.Variable(tf.zeros(shape), name=f'b_{layer_id}')\n",
    "        \n",
    "        return self.hidden_layers_b[layer_id]\n",
    "        \n",
    "    def get_dense_layer(self, layer_id, prev_layer, units_count, activ=tf.nn.relu):\n",
    "        input_size = prev_layer.get_shape().as_list()[1]\n",
    "        \n",
    "        W = self.get_W(layer_id, [input_size, units_count])\n",
    "        b = self.get_b(layer_id, [units_count])\n",
    "        \n",
    "        layer = tf.matmul(prev_layer, W) + b\n",
    "        \n",
    "        if activ:\n",
    "            layer = activ(layer, name=f'Lay_Dense_{layer_id}')\n",
    "        \n",
    "        return layer\n",
    "    \n",
    "    def get_conv2_layer(self, layer_id, prev_layer, kernel_size, output_channels, strides=1, padding='SAME', activ=tf.nn.relu):\n",
    "        input_channels = prev_layer.get_shape().as_list()[3]\n",
    "        \n",
    "        # 5x5 conv, 1 input, 32 outputs\n",
    "        # [filter_height, filter_width, in_channels, out_channels]\n",
    "        filter_shape = [kernel_size[0], kernel_size[1], input_channels, output_channels]\n",
    "\n",
    "        W = self.get_W(layer_id, filter_shape)\n",
    "        b = self.get_b(layer_id, [output_channels])\n",
    "        \n",
    "        layer = tf.nn.conv2d(prev_layer, W, [1, strides, strides, 1], padding) + b\n",
    "        \n",
    "        if activ:\n",
    "            layer = activ(layer, name=f'Lay_Conv2d_{layer_id}')  \n",
    "        \n",
    "        return layer\n",
    "    \n",
    "    def next_batch(self, x, y, batch_size, iteration):\n",
    "        start = iteration * batch_size\n",
    "        end = (iteration + 1) * batch_size\n",
    "        \n",
    "        return x[start:end], y[start:end]\n",
    "    \n",
    "    def flatten(self, input):\n",
    "        shape = input.get_shape().as_list()\n",
    "        shape = np.array(shape)\n",
    "        size = shape[shape != None].prod()\n",
    "        \n",
    "        return tf.reshape(input, [-1, size], name='Flatten')\n",
    "    \n",
    "    def get_max_pooling(self, input, ksize, stride=1, padding='SAME'):\n",
    "        return tf.nn.max_pool(input, [1, ksize, ksize, 1], [1, stride, stride, 1], padding, name='max_pool')\n",
    "\n",
    "    def get_avg_pooling(self, input, ksize, stride=1, padding='SAME'):\n",
    "        return tf.nn.avg_pool(input, [1, ksize, ksize, 1], [1, stride, stride, 1], padding, name='avg_pool')\n",
    "    \n",
    "    def pre_compile():\n",
    "        print('precompile')\n",
    "    \n",
    "    def compile(self):\n",
    "        \n",
    "        self.lr = tf.placeholder(tf.float32, shape=[])\n",
    "        \n",
    "        self.pre_compile()\n",
    "        \n",
    "        self.prediction = tf.nn.softmax(self.layer_output, name='Output')\n",
    "        self.accuracy = tf.reduce_mean(\n",
    "            tf.cast(\n",
    "                tf.equal(tf.argmax(self.prediction, 1), tf.argmax(self.expected_output, 1)),\n",
    "                tf.float32,\n",
    "            ),\n",
    "            name='Accuracy'\n",
    "        )\n",
    "                \n",
    "        self.session = tf.Session()\n",
    "        self.vars = tf.global_variables_initializer()\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def fit(self, x=None, y=None, batch_size=64, epochs=1):\n",
    "        \n",
    "        valid_size = 0.3\n",
    "        \n",
    "        if len(y) * valid_size > 20_000:\n",
    "            valid_size = 20_000\n",
    "        \n",
    "        x_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size=valid_size, random_state=50)\n",
    "        \n",
    "        print(f'Train size: {len(y_train)},\\t Valid size: {len(y_valid)}')\n",
    "        \n",
    "        iterations = int(len(y_train) / batch_size)\n",
    "        \n",
    "        display_info = int(iterations / 2)\n",
    "        \n",
    "        print(self.print_separator)\n",
    "        print(f'Epochs: {epochs}\\t| Iterations: {iterations}\\t| Batch: {batch_size}')\n",
    "        print(self.print_separator)\n",
    "        \n",
    "        self.session.run(self.decay_steps.assign(iterations))\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            x_train_epoch, y_train_epoch = resample(x_train, y_train)\n",
    "            \n",
    "            for iteration in range(iterations):\n",
    "                x_batch, y_batch = self.next_batch(x_train_epoch, y_train_epoch, batch_size, iteration)\n",
    "\n",
    "                feed_data = { \n",
    "                    self.input: x_batch, \n",
    "                    self.expected_output: y_batch,\n",
    "                    self.dropout_rate_tf: self.dropout_rate\n",
    "                }\n",
    "                \n",
    "                self.session.run(self.optimizer, feed_dict=feed_data)\n",
    "\n",
    "            \n",
    "            feed_data_train = { self.input: x_train, self.expected_output: y_train, self.dropout_rate_tf: 0}\n",
    "            loss_train, acc_train = self.session.run([self.loss, self.accuracy], feed_dict=feed_data_train)\n",
    "            \n",
    "            # print(self.session.run([self.global_step, self.decay_steps, self.lr]))\n",
    "            \n",
    "            self.history['acc_train'].append(acc_train)\n",
    "            self.history['loss_train'].append(loss_train)\n",
    "\n",
    "            feed_data_valid = { self.input: x_valid, self.expected_output: y_valid, self.dropout_rate_tf: 0}\n",
    "            loss_valid, acc_valid = self.session.run([self.loss, self.accuracy], feed_dict=feed_data_valid)\n",
    "\n",
    "            self.history['acc_valid'].append(acc_valid)\n",
    "            self.history['loss_valid'].append(loss_valid)\n",
    "\n",
    "            print(f'Epoch {epoch + 1}: loss - Tr[{loss_train:.2f}] Va[{loss_valid:.2f}] \\t acc - Tr[{acc_train:.01%}] Va[{acc_valid:.01%}]')\n",
    "            print(self.print_separator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T22:09:42.439933Z",
     "start_time": "2020-03-08T22:09:42.430160Z"
    }
   },
   "outputs": [],
   "source": [
    "class Model1CCD(BaseModel):\n",
    "    def __init__(self):\n",
    "        BaseModel.__init__(self)\n",
    "\n",
    "    def pre_compile(self):\n",
    "        self.reset_internal_params()\n",
    "        \n",
    "        self.input = tf.placeholder(tf.float32, shape=[None, self.input_size], name=\"Input\")\n",
    "        self.expected_output = tf.placeholder(tf.float32, shape=[None, self.output_size], name=\"Y_actual\")\n",
    "        \n",
    "        # [batch, in_height, in_width, in_channels]\n",
    "        layer_1_pre = tf.reshape(self.input, [-1, 28, 28, 1])\n",
    "        \n",
    "        layer_1 = self.get_conv2_layer(1, layer_1_pre, [3, 3], 16, strides=2)\n",
    "        \n",
    "        layer_2 = self.get_conv2_layer(2, layer_1, [3, 3], 16, strides=2)\n",
    "                    \n",
    "        layer_3_pre = self.flatten(layer_2)\n",
    "        \n",
    "        layer_3 = self.get_dense_layer(3, layer_3_pre, 50)\n",
    "        layer_3_do = tf.nn.dropout(layer_3, rate=self.dropout_rate_tf)\n",
    "        \n",
    "        self.layer_output = self.get_dense_layer(4, layer_3_do, 10, activ=None)\n",
    "        \n",
    "        self.loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.expected_output, logits=self.layer_output),\n",
    "            name='Loss'\n",
    "        )\n",
    "        \n",
    "        self.lr = tf.compat.v1.train.exponential_decay(self.start_learning_rate, self.global_step, self.decay_steps, 0.96)\n",
    "        \n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(self.lr).minimize(self.loss, global_step=self.global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T22:09:54.534889Z",
     "start_time": "2020-03-08T22:09:54.525741Z"
    }
   },
   "outputs": [],
   "source": [
    "class Model2CPD(BaseModel):\n",
    "    def __init__(self):\n",
    "        BaseModel.__init__(self)\n",
    "\n",
    "    def pre_compile(self):\n",
    "        self.reset_internal_params()\n",
    "        \n",
    "        self.input = tf.placeholder(tf.float32, shape=[None, self.input_size], name=\"Input\")\n",
    "        self.expected_output = tf.placeholder(tf.float32, shape=[None, self.output_size], name=\"Y_actual\")\n",
    "        \n",
    "        # [batch, in_height, in_width, in_channels]\n",
    "        layer_1_pre = tf.reshape(self.input, [-1, 28, 28, 1])\n",
    "        \n",
    "        layer_1 = self.get_conv2_layer(1, layer_1_pre, [5, 5], 16, strides=2)\n",
    "        \n",
    "        layer_1_pool = self.get_max_pooling(layer_1, 3)\n",
    "                    \n",
    "        layer_2_pre = self.flatten(layer_1_pool)\n",
    "        \n",
    "        layer_2 = self.get_dense_layer(3, layer_2_pre, 50)\n",
    "        layer_2_do = tf.nn.dropout(layer_2, rate=self.dropout_rate_tf)\n",
    "        \n",
    "        self.layer_output = self.get_dense_layer(4, layer_2_do, 10, activ=None)\n",
    "        \n",
    "        self.loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.expected_output, logits=self.layer_output),\n",
    "            name='Loss'\n",
    "        )\n",
    "        \n",
    "        self.lr = tf.compat.v1.train.exponential_decay(self.start_learning_rate, self.global_step, self.decay_steps, 0.96)\n",
    "        \n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(self.lr).minimize(self.loss, global_step=self.global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T22:45:46.674041Z",
     "start_time": "2020-03-08T22:45:44.280602Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object del\n",
      "Train size: 7000,\t Valid size: 3000\n",
      "-----------------------------------------------------------------\n",
      "Epochs: 1\t| Iterations: 54\t| Batch: 128\n",
      "-----------------------------------------------------------------\n",
      "Epoch 1: loss - Tr[2.24] Va[2.24] \t acc - Tr[38.6%] Va[39.6%]\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class LeNet5(BaseModel):\n",
    "    def __init__(self):\n",
    "        BaseModel.__init__(self)\n",
    "    \n",
    "    def init_basic_params(self):\n",
    "        super().init_basic_params()\n",
    "        self.input_size = [32, 32, 1]\n",
    "    \n",
    "    def pad_x(self, x):\n",
    "        temp = x.reshape(-1, 28, 28, 1)\n",
    "        return np.pad(temp, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "    \n",
    "    def fit(self, x=None, y=None, batch_size=64, epochs=1):\n",
    "        x_pad = self.pad_x(x)\n",
    "        super().fit(x=x_pad, y=y, batch_size=batch_size, epochs=epochs) \n",
    "    \n",
    "    def pre_compile(self):\n",
    "        self.reset_internal_params()\n",
    "\n",
    "        self.input = tf.placeholder(tf.float32, shape=[None, *self.input_size], name=\"Input\")\n",
    "        self.expected_output = tf.placeholder(tf.float32, shape=[None, self.output_size], name=\"Y_actual\")\n",
    "   \n",
    "        # Conv. out - 6, kernel - 5x5, stride = 1\n",
    "        layer_1 = self.get_conv2_layer(1, self.input, [5, 5], 6, strides=1, padding='VALID', activ=tf.nn.tanh)\n",
    "        \n",
    "        # avg pool. filter = 2x2, stride = 2\n",
    "        layer_2 = self.get_avg_pooling(layer_1, 2, stride=2, padding='VALID')\n",
    "        \n",
    "        # Conv. out - 16, kernel - 5x5, stride = 1\n",
    "        layer_3 = self.get_conv2_layer(3, layer_2, [5, 5], 16, strides=1, padding='VALID', activ=tf.nn.tanh)\n",
    "        \n",
    "        # avg pool. filter = 2x2, stride = 2\n",
    "        layer_4 = self.get_avg_pooling(layer_3, 2, stride=2, padding='VALID')\n",
    "        \n",
    "        # Conv. out - 16, kernel - 5x5, stride = 1\n",
    "        layer_5 = self.get_conv2_layer(5, layer_4, [5, 5], 120, strides=1, padding='VALID', activ=tf.nn.tanh)\n",
    "        \n",
    "        # The fifth layer (C5) is a fully connected convolutional layer \n",
    "        # with 120 feature maps each of size 1×1. Each of the 120 units in C5 is connected to all the 400 nodes \n",
    "        # (5x5x16) in the fourth layer S4.\n",
    "                    \n",
    "        #layer_5_pre = self.flatten(layer_4)\n",
    "        \n",
    "        #layer_5 = self.get_dense_layer(5, layer_5_pre, 84)\n",
    "        #layer_2_do = tf.nn.dropout(layer_2, rate=self.dropout_rate_tf)\n",
    "        \n",
    "        self.layer_output = self.get_dense_layer(4, layer_5, 10, activ=None)\n",
    "        \n",
    "        self.loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.expected_output, logits=self.layer_output),\n",
    "            name='Loss'\n",
    "        )\n",
    "        \n",
    "        self.lr = tf.compat.v1.train.exponential_decay(self.start_learning_rate, self.global_step, self.decay_steps, 0.96)\n",
    "        \n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(self.lr).minimize(self.loss, global_step=self.global_step)\n",
    "\n",
    "m3 = LeNet5()\n",
    "m3.fit(train_X, train_y, batch_size=128, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T22:35:29.015772Z",
     "start_time": "2020-03-08T22:35:29.013264Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T22:35:42.952572Z",
     "start_time": "2020-03-08T22:35:42.948777Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
